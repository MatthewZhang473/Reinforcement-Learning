{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "\n",
    "I am going to introduce it from two prospectives, the first is from Cambridge Lectures, the other from the Limu's book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambridge Version (Deterministic)\n",
    "\n",
    "#### Define Q value, and its properties.\n",
    "\n",
    "Let's look at why we define the action-value function (Q value).\n",
    "\n",
    "To find the optimum policy $\\pi^{*}(s)$, we need to maximize the value V:\n",
    "\n",
    "$\\pi^{*}(s) = a^{*}(s) = \\arg\\max_{a}[r(s,a) + V(f(s,a))]$\n",
    "\n",
    "Here f(s,a) is the **model** that tells us where does taking action a lead us to.\n",
    "\n",
    "But we don't know the model!!\n",
    "\n",
    "Hence, to go around it, let define: $Q(s,a) = r(s,a) + V(f(s,a))$.\n",
    "\n",
    "And we have: $ V(s) = max_{a}Q(s,a) $.\n",
    "\n",
    "Hence, by definition: \n",
    "\n",
    "$ Q(s,a) = r(s,a) + V(f(s,a)) = r(s,a) + max_{a'}Q(f(s,a'),a')$. \n",
    "\n",
    "This gives us the another recursion equation that converges to optimum (Bellman's equation).\n",
    "\n",
    "(Note we still need the model f() at this point.)\n",
    "\n",
    "With that, we can find the policy using $\\pi^{*}(s) = a^{*}(s) = \\max_{a} Q(s,a)$\n",
    "\n",
    "With discount factor $\\gamma$, we have:\n",
    "\n",
    "$ Q(s,a) = r(s,a) + \\gamma max_{a'}Q(f(s,a'),a')$\n",
    "\n",
    "#### Q-learning\n",
    "\n",
    "Take samples s(i), a(i), r(i), s(i+1)  Using the bellman's equation to converge the Q function:\n",
    "\n",
    "$Q_{k+1}(s(i),a(i)) = r(s(i),a(i)) + \\gamma max_{a'}Q_{k}(s(i+1),a')$\n",
    "\n",
    "And now we don't need to know the model, instead, we learn implicitly from sample data.\n",
    "\n",
    "For stochastic case, it make sense that we want to keep the $Q_{k}$ for a while, instead of all updating to the new value hence everything is probabalitic, hence:\n",
    "\n",
    "$Q_{k+1}(s(i),a(i)) = (1-\\alpha_{k}) Q_{k}(s(i), a(i)) +\\alpha_{k}[r(s(i),a(i)) + \\gamma max_{a'}Q_{k}(s(i+1),a')]$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limu's Version\n",
    "\n",
    "(Maybe not as strict, but the idea is right)\n",
    "\n",
    "If we have found the optimum policy $\\pi^{*}(s)$ and the optimum Q values, we should have:\n",
    "\n",
    "$Q(s(i),a(i)) = r(s(i),a(i)) + \\gamma\\max_{a'}(Q(s(i+1), a'))$\n",
    "\n",
    "If we now collect a a dataset along one trajectory {s(0), a(0), s(1), a(1) ...}, we can define a loss function as:\n",
    "\n",
    "$ L(Q) = \\sum_{i}[Q(s(i),a(i)) - r(s(i),a(i)) - \\gamma\\max_{a'}(Q(s(i+1), a'))]^{2} $\n",
    "\n",
    "From which we can minimize to find optimum Q:\n",
    "\n",
    "$  \\hat{Q} = \\min_{Q}L(Q)$\n",
    "\n",
    "Again, we do not explicitly need the transition function (model), yet we implicitly have it by sampling data.\n",
    "\n",
    "Now we use gradient descend:\n",
    "\n",
    "$Q_{k+1}(s(i), a(i)) \\leftarrow Q_{k}(s(i), a(i)) - \\alpha \\nabla_{Q}L(Q_{k})  \\\\ = (1-\\alpha_{k}) Q_{k}(s(i), a(i)) +\\alpha_{k}[r(s(i),a(i)) + \\gamma max_{a'}Q_{k}(s(i+1),a')]$\n",
    "\n",
    "To take in the account of the case when the trajectory ends, the value of the terminal state is zero because the robot does not any furhter actions:\n",
    "\n",
    "$Q_{k+1}(s(i), a(i)) \\leftarrow Q_{k}(s(i), a(i)) - \\alpha \\nabla_{Q}L(Q_{k})  \\\\ = (1-\\alpha_{k}) Q_{k}(s(i), a(i)) \\\\ +\\alpha_{k}[r(s(i),a(i)) + \\gamma (1 - 1_{s(i+1) is terminal})max_{a'}Q_{k}(s(i+1),a')]$\n",
    "\n",
    "Finally, with the solution of these updates $\\hat{Q}$, which is an approximation of the optimal value function $Q^{*}*$, we can recover the optimal policy easily:\n",
    "\n",
    "$\\hat{\\pi}(s) = \\arg\\max_{a}\\hat{Q}(s,a)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My question\n",
    "\n",
    "I don't get why do we have to use Q value. it seems like using Value function V can do the very same thing. E.g. converging by applying bellman's equation randomly.\n",
    "\n",
    "ChatGPT:\n",
    "\n",
    "Q learning is model free, V learning can be model free / model-based\n",
    "\n",
    "We are using Q-learning more because:\n",
    "1. More effective exploration (we will talk about soon, about data sampling)\n",
    "2. More detailed data (both action and state recorded)\n",
    "3. which leads to faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration in Q-learning\n",
    "\n",
    "It is cruicial that all states $s \\in S$ are visited, otherwise the estimateion of $Q^{*}$ will be bad as value iteration essentially ties together the value of all state-action pairs. It is therefore essential to pick the right $\\pi_{e}$ to collect the data.\n",
    "\n",
    "#### Epislon Greedy\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  \\pi_{e}(a|s) =\n",
    "  \\begin{cases}\n",
    "    \\arg\\max_{a'}\\hat{Q}(s,a'), & \\text{with prob $1-\\epsilon$}.\\\\\n",
    "    uniform(A), & \\text{with prob $\\epsilon$}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This ensures we are mostly choosing the action that maximize Q, yet leaving some space for random exploration. Good actions, e.g., whose value \n",
    "is large, are explored more often by the robot and thereby reinforced.\n",
    "\n",
    "Other exploration policy such as **softmax** also works simialrly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the same frozenlake example\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "\n",
    "seed = 0  # Random number generator seed\n",
    "gamma = 0.95  # Discount factor\n",
    "num_iters = 256  # Number of iterations\n",
    "alpha   = 0.9  # Learing rate\n",
    "epsilon = 0.9  # Epsilon in epsilion gready algorithm\n",
    "random.seed(seed)  # Set the random seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Now set up the environment\n",
    "env_info = d2l.make_env('FrozenLake-v1', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def e_greedy(env, Q, s, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "    else:\n",
    "        return np.argmax(Q[s,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(env_info, gamma, num_iters, alpha, epsilon):\n",
    "    env_desc = env_info['desc']  # 2D array specifying what each grid item means\n",
    "    env = env_info['env']  # 2D array specifying what each grid item means\n",
    "    num_states = env_info['num_states']\n",
    "    num_actions = env_info['num_actions']\n",
    "\n",
    "    Q  = np.zeros((num_states, num_actions))\n",
    "    V  = np.zeros((num_iters + 1, num_states))\n",
    "    pi = np.zeros((num_iters + 1, num_states))\n",
    "\n",
    "    for k in range(1, num_iters + 1):\n",
    "        # Reset environment\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            # Select an action for a given state and acts in env based on selected action\n",
    "            action = e_greedy(env, Q, state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Q-update:\n",
    "            y = reward + gamma * np.max(Q[next_state,:])\n",
    "            Q[state, action] = Q[state, action] + alpha * (y - Q[state, action])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "        # Record max value and max action for visualization purpose only\n",
    "        for s in range(num_states):\n",
    "            V[k,s]  = np.max(Q[s,:])\n",
    "            pi[k,s] = np.argmax(Q[s,:])\n",
    "    d2l.show_Q_function_progress(env_desc, V[:-1], pi[:-1])\n",
    "\n",
    "q_learning(env_info=env_info, gamma=gamma, num_iters=num_iters, alpha=alpha, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some remarks on implementation\n",
    "\n",
    "This Q-learning is much slower than previous method using value iteration, this is because we do not have access to the model (transition function / MDP)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
