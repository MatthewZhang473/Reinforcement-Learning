{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook contains the foundation of reinforcement learning, introduced several key ideas such as policy, action value, and value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "### Key Quantities:\n",
    "\n",
    "1. States (S)\n",
    "2. Actions (A)\n",
    "3. Transition Function (T): \n",
    "   We may not know which state the action will lead to at a given state (randomness)\n",
    "\n",
    "   Mathematically:\n",
    "\n",
    "   $T(s,a,s') = P(s'|s,a)$\n",
    "   \n",
    "   and it must satisfy:\n",
    "\n",
    "   $\\sum_{s'\\in S} T(s,a,s') = 1$\n",
    "4. Rewards (r):\n",
    "   Designed by user\n",
    "   \n",
    "   r(s,a) is the reward for taking action $a$ from state $s$\n",
    "\n",
    "These give us the MDP: (S,A,T,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return and Discount Factor\n",
    "\n",
    "\n",
    "Trajectory: $\\tau = (s_{0}, a_{0}, r_{0}, s_{1}, ...)$\n",
    "\n",
    "Return of a trajectory: $R(\\tau) = r_{0} + r_{1} + r_{2} ...$\n",
    "\n",
    "Objective: $\\underset{x}{\\arg\\max} \\sum r$\n",
    "\n",
    "To avoid infinite sum that make the comparision meaningless --> Discount factor $\\gamma$\n",
    "\n",
    "$R(\\tau) = r_{0} + \\gamma r_{1} + \\gamma^{2} r_{2} + ... = \\sum_{t=0}^{\\infty}\\gamma^{t} r_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why 'Markov'?\n",
    "\n",
    "Markov system: $s_{t+1}$ only depends on $s_{t}$ and $a_{t}$\n",
    "\n",
    "counter example: Newtonian system:\n",
    "\n",
    " $ s_{t+1} $ depends on acceleration $a_{t}$, position $s_{t}$, but also velocity (which is proportional to $(s_{t} - s_{t-1})$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:\n",
    "\n",
    "#### Q1:\n",
    "\n",
    "In [Mountain Car Problem](https://www.gymlibrary.dev/environments/classic_control/mountain_car/),\n",
    "\n",
    "The state space: [x,v] where $x,v \\in \\mathbb{R}$\n",
    "\n",
    "The action space: [0,1,2] corresponding to accelerating left, don't accelerate and accelerate right.\n",
    "\n",
    "Possible rewards: the distance to the terminal\n",
    "\n",
    "#### Q2:\n",
    "\n",
    "For atari game [pong](https://www.gymlibrary.dev/environments/atari/pong/)\n",
    "\n",
    "MDP design:\n",
    "1. States: [position of my pad, position of opponent's pad, position of ping pong]\n",
    "2. actions: [down, none, up]\n",
    "3. rewards: scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration\n",
    "\n",
    "**Stochastic Policy** $\\pi(a|s)$ is a categorical probability distributioin.\n",
    "\n",
    "\n",
    "e.g when action space is $[a_{0}, a_{1}, a_{2}]$, $\\pi(a_{i} | s_{0}) = [0.4, 0.2, 0.1, 0.3]$ (sum to unity)\n",
    "\n",
    "**Deterministic Policy** is a special case when there is only one non-zero probability. e.g. [0,0,1,0]\n",
    "\n",
    "Value function (V): (The expected return if policy $\\pi$ is used)\n",
    "\n",
    "$V^{\\pi}(s_{0}) = E_{a_{t} = \\pi(s_{t})}[R(\\tau)]$\n",
    "\n",
    "Remember that R is the discounted return: $R(\\tau) = r_{0} + \\gamma r_{1} + \\gamma^{2} r_{2} + ... = \\sum_{t=0}^{\\infty}\\gamma^{t} r_{t}$\n",
    "\n",
    "### Decomposit Value function:\n",
    "\n",
    "The Value ofstate $s_{0}$  = average reward obtained in the first stage and the value function averaged over all possible next states $s_{1}$ --> Basic of dynamic programming. \n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$V^{\\pi}(s_{0}) = r(s_{0}, a_{0}) + \\gamma E_{a_{0} = \\pi_{s_{0}}}(E_{s_{1} = P(s_{1}| s_{0},a_{0})}(V^{\\pi}(s_{1})))$\n",
    "\n",
    "Notice that the second stage gets two expectations, one over the choices of the action taken in the first stage using the stochastic policy and another over the possible states obtained from the chosen action $a_{0}$\n",
    "\n",
    "Using the transition probabilities in MDP, we write:\n",
    "\n",
    "$ V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)[r(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V^{\\pi}(s') ]$, for all $s \\in S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Value Function\n",
    "\n",
    "Action Value Function (Q):\n",
    "\n",
    "The average return of a trajectory that begins at $s_{0}$ but when the action of the first stage is fixed to be $a_{0}$\n",
    "\n",
    "$Q^{\\pi}(s_{0},a_{0}) = r(s_{0}, a_{0}) + E_{a_{t} = \\pi(s_{t})}[\\sum_{t=1}^{\\infty}\\gamma^{t} r_{t}]$\n",
    "\n",
    "Note that the summation start from t = 1.\n",
    "\n",
    "Again, using the transition probability:\n",
    "\n",
    "$Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a) \\sum_{a \\in A}\\pi(a'|s') Q^{\\pi}(s',a')$\n",
    "\n",
    "Note:\n",
    "1.  The first summation is the expectation of taking s' arrived after taking action s.\n",
    "2.  The second summation is the expectation of further Q values with probability of actions following the policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Stochastic Policy\n",
    "\n",
    "Optimal policy: $\\pi^{*}  = \\arg \\max_{\\pi}V^{\\pi}(s_{0}) $\n",
    "\n",
    "Denote the value function and the action-value function of the optimal policy as $V^{*} = V^{\\pi^{*}}$, and $Q^{*} = Q^{\\pi^{*}}$\n",
    "\n",
    "For the case of deterministic policy:\n",
    "\n",
    "$\\pi^{*}(s) = \\arg\\max_{a \\in A}[r(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V^{*}(s')]$\n",
    "\n",
    "A good mnemonic to remember this is that the optimal action at state $s$ (for a deterministic policy) is the one that maximizes the sum of reward $r(s,a)$ from the first stage and the average return of the trajectories starting from the next sate \n",
    "$s'$, averaged over all possible next states $s'$ from the second stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle of Dynamic Programming\n",
    "\n",
    "In the case of optimum policy $\\pi^{*}$, we have\n",
    "\n",
    "$ V^{*}(s) = \\sum_{a \\in A} \\pi^{*}(a|s)[r(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V^{\\pi}(s') ]$, for all $s \\in S$\n",
    "\n",
    "For deterministic optimal policy $\\pi^{*}$, since there is only one action that can be taken at the state s, we can also write:\n",
    "\n",
    "$ V^{*}(s) = \\arg\\max_{a \\in A}[r(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V^{\\pi}(s') ]$, for all $s \\in S$\n",
    "\n",
    "(You can think about $\\arg\\max_{a \\in A}$ as finding the $\\pi^{*}$)\n",
    "\n",
    "This, is the bellman equation, the principle of DP. We can remember it as \"the remainder of an optimal trajectory is also optimal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Turn principle of dynamic programming into an algorithm for **finding the optimal value function** by iteration. (e.g. shortest path)\n",
    "\n",
    "$ V_{k+1}(s) = \\max_{a \\in A} [r(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V_{k}(s') ]$, for all $s \\in S$\n",
    "\n",
    "For $k\\to \\infty$, the Value function converges to the optimal value, irrespective of the initialization $V_{0}$.\n",
    "\n",
    "In mathematics words: $V^{*}(s) = \\lim_{k\\to\\infty}V_{k}(s)$\n",
    "\n",
    "Similarly for action value function\n",
    "\n",
    "$Q_{k+1}(s,a) = r(s,a) + \\gamma V_{k}(s') =  r(s,a) + \\gamma \\max_{a' \\in A}\\sum_{s' \\in S}P(s'|s,a) Q_{k}(s',a')$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "\n",
    "Value iteration also enables us to compute the optimal values, given the policy $\\pi$.\n",
    "\n",
    "$ V^{\\pi}_{k+1}(s) = \\sum_{a \\in A} \\pi(a|s)[r(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V^{\\pi}_{k}(s') ]$, for all $s \\in S$\n",
    "\n",
    "This is known as **policy evaluation**. It converges to the correct value function irrespective of the initalization $V_{0}$.\n",
    "\n",
    "(Think about using this to evaluate the performance of your policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with [Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Value Iteration\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "\n",
    "seed = 0  # Random number generator seed\n",
    "gamma = 0.95  # Discount factor\n",
    "num_iters = 10  # Number of iterations\n",
    "random.seed(seed)  # Set the random seed to ensure results can be reproduced\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Now set up the environment\n",
    "env_info = gym.make_env('FrozenLake-v1', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env_info, gamma, num_iters):\n",
    "    env_desc = env_info['desc']  # 2D array shows what each item means\n",
    "    prob_idx = env_info['trans_prob_idx']\n",
    "    nextstate_idx = env_info['nextstate_idx']\n",
    "    reward_idx = env_info['reward_idx']\n",
    "    num_states = env_info['num_states']\n",
    "    num_actions = env_info['num_actions']\n",
    "    mdp = env_info['mdp']\n",
    "\n",
    "    V  = np.zeros((num_iters + 1, num_states))\n",
    "    Q  = np.zeros((num_iters + 1, num_states, num_actions))\n",
    "    pi = np.zeros((num_iters + 1, num_states))\n",
    "\n",
    "    for k in range(1, num_iters + 1):\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                # Calculate \\sum_{s'} p(s'\\mid s,a) [r + \\gamma v_k(s')]\n",
    "                for pxrds in mdp[(s,a)]:\n",
    "                    # mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]\n",
    "                    pr = pxrds[prob_idx]  # p(s'\\mid s,a)\n",
    "                    nextstate = pxrds[nextstate_idx]  # Next state\n",
    "                    reward = pxrds[reward_idx]  # Reward\n",
    "                    Q[k,s,a] += pr * (reward + gamma * V[k - 1, nextstate])\n",
    "            # Record max value and max action\n",
    "            V[k,s] = np.max(Q[k,s,:])\n",
    "            pi[k,s] = np.argmax(Q[k,s,:])\n",
    "    gym.show_value_function_progress(env_desc, V[:-1], pi[:-1])\n",
    "\n",
    "value_iteration(env_info=env_info, gamma=gamma, num_iters=num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Q1: Increase the grid size to 8x8\n",
    "\n",
    "Q2: Computation complexity of Value iteration\n",
    "\n",
    "Q3: Change gamma to 0, 0.5,1\n",
    "\n",
    "Q4: How does gamma affect the number of iterations needed to converge? What happens when gamma = 1?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
